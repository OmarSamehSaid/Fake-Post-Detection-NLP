{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bff62f3",
   "metadata": {},
   "source": [
    "## Data mining questions\n",
    "\n",
    "1- What is the difference between Character n-gram and Word n-gram? Which one tends to suffer more from the OOV issue?\n",
    "\n",
    "- The main difference between character n-gram and word n-gram is that character n-gram is based on individual characters, whereas word n-gram is based on whole words\n",
    "- In terms of the OOV (out-of-vocabulary) issue, character n-gram tends to suffer less than word n-gram. This is because character n-gram can capture sub-word information\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "2- What is the difference between stop word removal and stemming? Are these techniques language-dependent?\n",
    "\n",
    "- the main difference between the two is that stop word removal involves completely eliminating predetermined words that are listed, while stemming involves reducing words to their roots by removing prefixes and suffixes, without removing the entire word.\n",
    "- These techniques can be language-dependent as stop words and word inflections can vary across different languages\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "3- Is tokenization techniques language dependent? Why?\n",
    "- Yes, tokenization techniques are language-dependent, as different languages have their own unique rules and structures for dividing text into individual units or tokens.\n",
    "\n",
    "---\n",
    "\n",
    "4- What is the difference between count vectorizer and tf-idf vectorizer? Would it be feasible to use all possible n-grams? If not, how should you select them?\n",
    "\n",
    "- Count vectorizer is a simple technique that converts a collection of text documents into a matrix of token counts\n",
    "- TF-IDF (term frequency-inverse document frequency) vectorizer, on the other hand, is a more advanced technique that assigns weights to the words based on their importance in the text\n",
    "- it may not be feasible to use all possible n-grams as the number of features can become very large, leading to the curse of dimensionality and computational complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70de71d",
   "metadata": {},
   "source": [
    "## problem formulation\n",
    "</br>\n",
    "\n",
    "### problem definition\n",
    "\n",
    "- our problem here is about building a model to classify and detect which real news and fake news from just its titles, our inputs here are news titles (60001) observations for a training dataset and the output is (60001) label.\n",
    "---\n",
    "\n",
    "### Data mining function\n",
    "- text preprocessing -> tokenization and vectorization each text ->training the model -> classification and prediction.\n",
    "\n",
    "---\n",
    "\n",
    "### Challenges\n",
    "\n",
    "- We have a very big data set so maybe will take some time to preprocess it and train it.\n",
    "\n",
    "- The input data consists of unclean text observations that contain numerous punctuation marks, non-English letters, misspellings, and grammatical errors. These titles were typed by humans, thus requiring an appropriate text cleaning technique to be selected. To determine the optimal method, various techniques will be evaluated, and the one that produces the best results will be chosen.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Model impact\n",
    "\n",
    "- prevent the spreading of rumors on social media quickly. \n",
    "\n",
    "---\n",
    "\n",
    "### The ideal solution\n",
    "\n",
    "- at the end i got 0.83 using deep learning approach on kaggle while svm got me 0.85 and logistic regression 0.84 that was maximum i could get with the given time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb324081",
   "metadata": {},
   "source": [
    "## Experimental protocol\n",
    "\n",
    "### Data preprocessing\n",
    "---\n",
    "- cleaning input data for both training set and test set using stemming or lemmatizing each text.\n",
    "- removing label 2\n",
    "- using standard scaling\n",
    "\n",
    "---\n",
    "### Building models\n",
    "\n",
    "- Building our piplines (has the vectorizer and machine learning model)\n",
    "- Build the search speace and search for the best hyperparameters combinations but trying many fits.\n",
    "\n",
    "- At the end i tried deep nural network with lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ac1a247",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import nltk \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "pd.options.display.max_columns = 100\n",
    "pd.options.display.max_rows = 100\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb7ea75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4a71506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea0bfe8a",
   "metadata": {},
   "source": [
    "# read training dataset and test dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac1d6231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>265723</th>\n",
       "      <td>A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284269</th>\n",
       "      <td>British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207715</th>\n",
       "      <td>In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551106</th>\n",
       "      <td>Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8584</th>\n",
       "      <td>Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70046</th>\n",
       "      <td>Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189377</th>\n",
       "      <td>Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93486</th>\n",
       "      <td>Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140950</th>\n",
       "      <td>Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34509</th>\n",
       "      <td>Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59768 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                       text  \\\n",
       "id                                                                                                            \n",
       "265723  A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...   \n",
       "284269  British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...   \n",
       "207715  In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...   \n",
       "551106  Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...   \n",
       "8584    Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...   \n",
       "...                                                                                                     ...   \n",
       "70046                 Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)   \n",
       "189377                Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back   \n",
       "93486                 Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no   \n",
       "140950                Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)   \n",
       "34509                 Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep   \n",
       "\n",
       "        label  \n",
       "id             \n",
       "265723      0  \n",
       "284269      0  \n",
       "207715      0  \n",
       "551106      0  \n",
       "8584        0  \n",
       "...       ...  \n",
       "70046       0  \n",
       "189377      1  \n",
       "93486       0  \n",
       "140950      0  \n",
       "34509       1  \n",
       "\n",
       "[59768 rows x 2 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('xy_train.csv',index_col='id')\n",
    "data = data.drop(data[data.label == 2].index)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f7eec69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('x_test.csv',index_col='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37c3ee0",
   "metadata": {},
   "source": [
    "# text cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8338a0",
   "metadata": {},
   "source": [
    "- First method will clean text and will lemmtize\n",
    "- Second method will clean text and will stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3bd7c27c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'on', 'can', 'during', 't', 'his', 'into', 'ain', 'do', 'up', 'own', 'where', 'that', 'doing', 'after', 'weren', 'an', \"it's\", 'because', 'don', 'for', 'haven', 'ours', 'before', 'she', 'aren', \"mightn't\", 'against', \"you're\", 'who', 'then', 'been', 'have', 'some', 'was', 'himself', 'shouldn', 'more', 'further', 'did', 'all', 'm', 'no', 'there', 'needn', 'how', \"haven't\", 'should', 'her', 'of', 'him', 'our', 'so', 'herself', \"you've\", 'only', 'whom', 'were', 'doesn', 'its', 'by', 'are', \"hasn't\", 'o', 'over', 'your', 's', \"should've\", \"wasn't\", 'to', 'as', 'we', \"weren't\", 'yourselves', 'and', 'when', \"mustn't\", \"won't\", 're', 'these', 'a', 'between', 'll', \"you'll\", 'shan', 'what', 'being', \"hadn't\", \"wouldn't\", 'wasn', 'if', 'in', 'be', 'ourselves', 'this', 'again', 'such', 'until', 'hasn', 'they', 'while', \"that'll\", 'very', 'about', \"she's\", 'through', 'had', 'does', 'or', 'mightn', \"shouldn't\", 'too', 'theirs', 'once', 'is', 'here', \"don't\", 'below', 'those', 've', 'with', 'you', 'under', 'y', 'didn', 'd', 'won', 'just', \"you'd\", 'other', \"needn't\", \"doesn't\", 'ma', \"isn't\", 'i', 'any', 'couldn', 'has', 'off', 'their', 'than', 'them', 'out', 'same', 'not', \"aren't\", \"shan't\", 'it', 'me', 'above', 'hadn', 'themselves', 'will', 'now', 'hers', 'having', 'down', 'he', 'wouldn', 'yourself', 'the', \"couldn't\", 'why', 'both', 'am', 'each', 'most', 'my', 'yours', 'which', 'few', 'nor', 'isn', 'mustn', 'myself', 'itself', 'but', \"didn't\", 'at', 'from'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('omw-1.4')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "print(stop_words)\n",
    "\n",
    "def cleaning_text(text, for_embedding):\n",
    "    \"\"\"\n",
    "        - remove any html tags (< /br> often found)\n",
    "        - Keep only ASCII + European Chars and whitespace, no digits\n",
    "        - remove single letter chars\n",
    "        - convert all whitespaces (tabs etc.) to single wspace\n",
    "\n",
    "    \"\"\"\n",
    "    RE_WSPACE = re.compile(r\"\\s+\", re.IGNORECASE) #match one or more white sepace\n",
    "    RE_TAGS = re.compile(r\"<.*?>\") #match <any num of words>\n",
    "    RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž0-9]+\", re.IGNORECASE) #match any English word\n",
    "    RE_SINGLECHAR = re.compile(r\"\\b^[^A-Za-zÀ-ž0-9]+\\b\", re.IGNORECASE) #match any word with word boundary.\n",
    "    if for_embedding:\n",
    "        # Keep punctuation\n",
    "        RE_ASCII = re.compile(r\"[^A-Za-zÀ-ž,.!? ]\", re.IGNORECASE) #match any English word and any punctuation\n",
    "        RE_SINGLECHAR = re.compile(r\"\\b[A-Za-zÀ-ž,.!?]\\b\", re.IGNORECASE) #match any word and any punctuation with word boundary.\n",
    "\n",
    "    text = re.sub(RE_TAGS, \" \", text) #remove one or more white sepace\n",
    "    text = re.sub(RE_ASCII, \" \", text) #remove <any num of words>\n",
    "    text = re.sub(RE_SINGLECHAR, \" \", text) #remove any English word\n",
    "    text = re.sub(RE_WSPACE, \" \", text) #remove any word with word boundary.\n",
    "    word_tokens = word_tokenize(text)\n",
    "    \n",
    "    return word_tokens\n",
    "\n",
    "def lemmatize_clean_text(text ,for_embedding=False):\n",
    "\n",
    "    \"\"\" steps:\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and lemmatize\n",
    "    \"\"\"\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    word_tokens = cleaning_text(text, for_embedding)\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming or lemmatization, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "        words_filtered = [lemmatizer.lemmatize(word) for word in words_tokens_lower if word not in stop_words]\n",
    "\n",
    "    clean_text = \" \".join(words_filtered)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "\n",
    "def stemming_clean_text(text ,for_embedding=False):\n",
    "    \"\"\" steps:\n",
    "        if not for embedding (but e.g. tdf-idf):\n",
    "        - all lowercase\n",
    "        - remove stopwords, punctuation and stemming\n",
    "    \"\"\"\n",
    "\n",
    "    word_tokens = cleaning_text(text,for_embedding)\n",
    "\n",
    "    if for_embedding:\n",
    "        # no stemming or lemmatization, lowering and punctuation / stop words removal\n",
    "        words_filtered = word_tokens\n",
    "    else:\n",
    "        words_tokens_lower = [word.lower() for word in word_tokens]\n",
    "        words_filtered = [stemmer.stem(word) for word in words_tokens_lower if word not in stop_words]\n",
    "\n",
    "    clean_text = \" \".join(words_filtered)\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e922ff6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "265723    A group of friends began to volunteer at a homeless shelter after their neighbors protested. \"Se...\n",
       "284269    British Prime Minister @Theresa_May on Nerve Attack on Former Russian Spy: \"The government has c...\n",
       "207715    In 1961, Goodyear released a kit that allows PS2s to be brought to heel. https://m.youtube.com/w...\n",
       "551106    Happy Birthday, Bob Barker! The Price Is Right Host on How He'd Like to Be Remembered | \"As the ...\n",
       "8584      Obama to Nation: 聙\"Innocent Cops and Unarmed Young Black Men Should Not be Dying Before Magic Jo...\n",
       "                                                         ...                                                 \n",
       "70046                   Finish Sniper Simo H盲yh盲 during the invasion of Finland by the USSR (1939, colorized)\n",
       "189377                  Nigerian Prince Scam took $110K from Kansas man; 10 years later, he's getting it back\n",
       "93486                   Is It Safe To Smoke Marijuana During Pregnancy? You鈥檇 Be Surprised Of The Answer | no\n",
       "140950                  Julius Caesar upon realizing that everyone in the room has a knife except him (44 bc)\n",
       "34509                   Jeff Bridges Releasing 鈥楽leeping Tapes,鈥?a New Album Designed to Help You Fall Asleep\n",
       "Name: text, Length: 59768, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b0fc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_lemmatized = data[\"text\"].map(lambda x: lemmatize_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## clean and lemmatiz training set\n",
    "data_stemmed = data[\"text\"].map(lambda x: stemming_clean_text(x, for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and stemming training set\n",
    "test_lemmatized = test[\"text\"].map(lambda x: lemmatize_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and lemmatizing test set\n",
    "test_stemmed = test[\"text\"].map(lambda x: stemming_clean_text(x ,for_embedding=False) if isinstance(x, str) else x).copy() ## word cleaning and lemmatizing test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b2084a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "265723    group friend began volunteer homeless shelter neighbor protested seeing another person also need...\n",
       "284269    british prime minister theresa may nerve attack former russian spy government concluded highly l...\n",
       "207715    1961 goodyear released kit allows ps2s brought heel http youtube com watch v alxulk0t8cg 0 72 0 ...\n",
       "551106    happy birthday bob barker price right host like remembered man said ave pet spayed neutered 0 92...\n",
       "8584      obama nation innocent cop unarmed young black men dying magic johnson 1 0 0 2 1 jimbobshawobodob...\n",
       "                                                         ...                                                 \n",
       "70046                                            finish sniper simo h yh invasion finland ussr 1939 colorized\n",
       "189377                                    nigerian prince scam took 110k kansa man 10 year later getting back\n",
       "93486                                                         safe smoke marijuana pregnancy surprised answer\n",
       "140950                                          julius caesar upon realizing everyone room knife except 44 bc\n",
       "34509                                  jeff bridge releasing leeping tape new album designed help fall asleep\n",
       "Name: text, Length: 59768, dtype: object"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a25b11ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "265723    group friend began volunteer homeless shelter neighbor protested seeing another person also need...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_lemmatized.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f25c1ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "265723    group friend began volunt homeless shelter neighbor protest see anoth person also need natur lik...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_stemmed.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2572d80d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0            4307\n",
       "year         4121\n",
       "one          3285\n",
       "new          2998\n",
       "like         2949\n",
       "man          2705\n",
       "trump        2577\n",
       "u            2513\n",
       "colorized    2430\n",
       "people       2315\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Frequency of most common words in data_lemmatized\n",
    "word_freq_lemmatized = pd.Series(\" \".join(data_lemmatized).split()).value_counts()\n",
    "word_freq_lemmatized[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91fdfef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        4293\n",
       "year     4125\n",
       "one      3285\n",
       "like     3128\n",
       "new      2998\n",
       "look     2847\n",
       "color    2737\n",
       "man      2728\n",
       "get      2602\n",
       "trump    2578\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Frequency of most common words in data_stemmed\n",
    "word_freq_stemmed = pd.Series(\" \".join(data_stemmed).split()).value_counts()\n",
    "word_freq_stemmed[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7bc8d15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.538281\n",
       "1    0.461719\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562af120",
   "metadata": {},
   "source": [
    "here i tried to convert the label 2 to 0 but it gave lower accuracy than if i removed it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "096b784b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[\"label\"] = [0 if x == 2 else x for x in data[\"label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dad36dc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.538281\n",
       "1    0.461719\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"label\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1facbb29",
   "metadata": {},
   "source": [
    "# Stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fbd76b",
   "metadata": {},
   "source": [
    "## trial 0 first thing i wanted to try was Logistic regression (word analyzer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6a1d15c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('lg',\n",
       "                                              LogisticRegression(max_iter=10000,\n",
       "                                                                 n_jobs=-1,\n",
       "                                                                 random_state=42))]),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'tfidf__analyzer': ['word'],\n",
       "                                        'tfidf__max_df': array([0.2]),\n",
       "                                        'tfidf__min_df': array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33...\n",
       "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
       "                                                               (1, 4), (1, 5)],\n",
       "                                        'tfidf__smooth_idf': [False, True],\n",
       "                                        'tfidf__strip_accents': [None, 'ascii',\n",
       "                                                                 'unicode'],\n",
       "                                        'tfidf__sublinear_tf': [True]},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_lg = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"lg\", LogisticRegression(max_iter=10000,random_state=42,n_jobs=-1))])\n",
    "\n",
    "\n",
    "# define parameter space to test\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
    "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    \"tfidf__analyzer\": ['word'],\n",
    "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
    "    'tfidf__smooth_idf':[False,True],\n",
    "    \"tfidf__sublinear_tf\":[True]\n",
    "}\n",
    "\n",
    "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
    "\n",
    "pipe_lg_clf = RandomizedSearchCV(pipe_lg, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", n_iter=100, verbose=1)\n",
    "pipe_lg_clf.fit(data_stemmed, data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7cf094a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8776370315149837\n",
      "best params {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 6, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_lg_clf.best_score_))\n",
    "print('best params {}'.format(pipe_lg_clf.best_params_))\n",
    "# best score 0.8776370315149837\n",
    "# best params {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 6, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2a5d929d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('lg',\n",
       "                                              LogisticRegression(max_iter=10000,\n",
       "                                                                 n_jobs=-1,\n",
       "                                                                 random_state=42))]),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'lg__C': [1.0, 0.1, 0.005, 1.5, 2.0,\n",
       "                                                  3.5, 4, 5],\n",
       "                                        'lg__class_weight': ['balanced', None],\n",
       "                                        'lg__fit_intercept': [False, True],\n",
       "                                        'lg__solver': ['sag', 'newton-cg',\n",
       "                                                       'lbfgs', 'liblinear',\n",
       "                                                       'saga'],\n",
       "                                        'tfidf__analyzer': ['word'],\n",
       "                                        'tfidf__max_df': [0.2],\n",
       "                                        'tfidf__min_df': [6],\n",
       "                                        'tfidf__ngram_range': [(1, 5)],\n",
       "                                        'tfidf__smooth_idf': [True],\n",
       "                                        'tfidf__strip_accents': [None],\n",
       "                                        'tfidf__sublinear_tf': [False]},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'tfidf__sublinear_tf':[True],\n",
    "    'tfidf__strip_accents':[None],\n",
    "    'tfidf__smooth_idf':[True],\n",
    "    'tfidf__ngram_range': [(1, 4)],\n",
    "    'tfidf__analyzer':['word'],\n",
    "    'tfidf__min_df': [6], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'lg__class_weight':['balanced',None],\n",
    "    \"lg__solver\" : ['sag','newton-cg', 'lbfgs','liblinear','saga'],\n",
    "    'lg__C': [1.0,0.1,0.005,1.5,2.0,3.5,4,5],\n",
    "    'lg__fit_intercept':[False, True]\n",
    "\n",
    "}\n",
    "\n",
    "pipe_lg_clf = RandomizedSearchCV(pipe_lg, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", n_iter=200, verbose=1)\n",
    "pipe_lg_clf.fit(data_stemmed, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "000c1506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8784508358625386\n",
      "best score {'tfidf__sublinear_tf': False, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 6, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word', 'lg__solver': 'lbfgs', 'lg__fit_intercept': False, 'lg__class_weight': 'balanced', 'lg__C': 2.0}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_lg_clf.best_score_))\n",
    "print('best params {}'.format(pipe_lg_clf.best_params_))\n",
    "#best score 0.8784508358625386\n",
    "#{'tfidf__sublinear_tf': False, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 6, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word', 'lg__solver': 'lbfgs', 'lg__fit_intercept': False, 'lg__class_weight': 'balanced', 'lg__C': 2.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7daeb02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59151\n"
     ]
    }
   ],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = test_stemmed.index\n",
    "print(len(test['text']))\n",
    "submission['label'] = pipe_lg_clf.predict_proba(test_stemmed)[:,1]\n",
    "submission.to_csv('lg_stem2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c3839f",
   "metadata": {},
   "source": [
    "it was good enough but i think it still can be better but it will take very long time to tune using grid search so for next trial i wanted to try xgboost (char analyzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2daa10",
   "metadata": {},
   "source": [
    "## trial 1 XGBoost (char) i expect it to be above 80 also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5dad70da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('xgb',\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            callbacks=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            early_stopping_rounds=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            eval_metric=None,\n",
       "                                                            feature_types=None,\n",
       "                                                            gamma=None,\n",
       "                                                            gpu_id=None,\n",
       "                                                            grow_policy=None,\n",
       "                                                            importance_t...\n",
       "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
       "                                                               (1, 4), (1, 5)],\n",
       "                                        'tfidf__smooth_idf': [False, True],\n",
       "                                        'tfidf__strip_accents': [None, 'ascii',\n",
       "                                                                 'unicode'],\n",
       "                                        'tfidf__sublinear_tf': [True, False]},\n",
       "                   scoring='roc_auc')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_xgb = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"xgb\", XGBClassifier(random_state=42,n_jobs=-1))])\n",
    "\n",
    "\n",
    "# define parameter space to test\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
    "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
    "    'tfidf__analyzer':['word','char','char_wb'],\n",
    "    'tfidf__smooth_idf':[False,True],\n",
    "    \"tfidf__sublinear_tf\":[True,False]\n",
    "}\n",
    "\n",
    "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
    "\n",
    "pipe_xgb_clf = RandomizedSearchCV(pipe_xgb, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", n_iter=50, verbose=1)\n",
    "pipe_xgb_clf.fit(data_stemmed, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c71895dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8545179954285592\n",
      "best score {'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 61, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'char'}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_xgb_clf.best_score_))\n",
    "print('best score {}'.format(pipe_xgb_clf.best_params_))\n",
    "#0.8545179954285592\n",
    "#{'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 61, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'char'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2b83df8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('xgb',\n",
       "                                              XGBClassifier(base_score=None,\n",
       "                                                            booster=None,\n",
       "                                                            callbacks=None,\n",
       "                                                            colsample_bylevel=None,\n",
       "                                                            colsample_bynode=None,\n",
       "                                                            colsample_bytree=None,\n",
       "                                                            early_stopping_rounds=None,\n",
       "                                                            enable_categorical=False,\n",
       "                                                            eval_metric=None,\n",
       "                                                            feature_types=None,\n",
       "                                                            gamma=None,\n",
       "                                                            gpu_id=None,\n",
       "                                                            grow_policy=None,\n",
       "                                                            importance_t...\n",
       "                                        'tfidf__ngram_range': [(1, 5)],\n",
       "                                        'tfidf__smooth_idf': [False],\n",
       "                                        'tfidf__strip_accents': ['unicode'],\n",
       "                                        'tfidf__sublinear_tf': [False],\n",
       "                                        'xgb__booster': ['gbtree', 'gblinear',\n",
       "                                                         'dart'],\n",
       "                                        'xgb__gamma': [0, 0.1, 0.2],\n",
       "                                        'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
       "                                        'xgb__max_depth': [5, 8, 10, 15, 30,\n",
       "                                                           60],\n",
       "                                        'xgb__n_estimators': [400, 450, 500,\n",
       "                                                              600],\n",
       "                                        'xgb__subsample': [0.8, 0.9, 1]},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'tfidf__sublinear_tf':[False], \n",
    "    'tfidf__strip_accents':['unicode'],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 5)], #(1,2)\n",
    "    'tfidf__min_df': [6], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'tfidf__analyzer':['char'],\n",
    "    'xgb__booster':['gbtree','gblinear', 'dart'],\n",
    "    'xgb__n_estimators': [400,450, 500,600],\n",
    "    'xgb__max_depth': [5,8,10,15,30,60],\n",
    "    'xgb__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'xgb__gamma': [0, 0.1, 0.2],\n",
    "    'xgb__subsample': [0.8, 0.9, 1]\n",
    "\n",
    "}\n",
    "   \n",
    "pipe_xgb_clf = RandomizedSearchCV(pipe_xgb, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", n_iter=10, verbose=1)\n",
    "pipe_xgb_clf.fit(data_stemmed, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e6fc9fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8741512318381581\n",
      "best score {'xgb__subsample': 0.8, 'xgb__n_estimators': 500, 'xgb__max_depth': 60, 'xgb__learning_rate': 0.1, 'xgb__gamma': 0.1, 'xgb__booster': 'dart', 'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 6, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'char'}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_xgb_clf.best_score_))\n",
    "print('best score {}'.format(pipe_xgb_clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "dde0a6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59151\n"
     ]
    }
   ],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = test_stemmed.index\n",
    "print(len(test['text']))\n",
    "submission['label'] = pipe_xgb_clf.predict_proba(test_stemmed)[:,1]\n",
    "submission.to_csv('xgb_stem2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd59581",
   "metadata": {},
   "source": [
    "it toke very long but it was lower than logestic regression :("
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e47598",
   "metadata": {},
   "source": [
    "## trial 2 SVM \n",
    "i guess it should out preform logistic regression if we searched for best hyper parameter using grid but svm toke nearly longer time than any other model but i think it has the potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cb9a193b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                       ('svc',\n",
       "                                        SVC(probability=True,\n",
       "                                            random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'tfidf__analyzer': ['word'], 'tfidf__max_df': [0.2],\n",
       "                         'tfidf__min_df': [5], 'tfidf__ngram_range': [(1, 2)],\n",
       "                         'tfidf__smooth_idf': [False],\n",
       "                         'tfidf__strip_accents': ['unicode'],\n",
       "                         'tfidf__sublinear_tf': [True]},\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_svc = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"svc\", SVC(random_state=42,probability=True))])\n",
    "\n",
    "\n",
    "# define parameter space to test\n",
    "params = {\n",
    "    'tfidf__sublinear_tf':[True], \n",
    "    'tfidf__strip_accents':['unicode'],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 2)], #(1,2)\n",
    "    'tfidf__min_df': [5], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'tfidf__analyzer':['word']\n",
    "}\n",
    "\n",
    "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
    "\n",
    "pipe_svc_clf = GridSearchCV(pipe_svc, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", verbose=1)\n",
    "pipe_svc_clf.fit(data_stemmed, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b0793eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8810183459981413\n",
      "best score {'tfidf__analyzer': 'word', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 2), 'tfidf__smooth_idf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__sublinear_tf': True}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_svc_clf.best_score_))\n",
    "print('best score {}'.format(pipe_svc_clf.best_params_))\n",
    "#best score 0.8810183459981413\n",
    "#{'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 5, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66040681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59151\n"
     ]
    }
   ],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = test_stemmed.index\n",
    "print(len(test['text']))\n",
    "submission['label'] = pipe_svc_clf.predict_proba(test_stemmed)[:,1]\n",
    "submission.to_csv('svc_stem_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "968e819d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('svc', SVC(random_state=42))]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'svc__C': [0.01, 0.1, 1, 10, 100],\n",
       "                                        'svc__gamma': [0.5, 0.1, 1],\n",
       "                                        'svc__kernel': ['linear', 'poly', 'rbf',\n",
       "                                                        'sigmoid'],\n",
       "                                        'tfidf__analyzer': ['word'],\n",
       "                                        'tfidf__max_df': [0.2],\n",
       "                                        'tfidf__min_df': [5],\n",
       "                                        'tfidf__ngram_range': [(1, 2)],\n",
       "                                        'tfidf__smooth_idf': [False],\n",
       "                                        'tfidf__strip_accents': ['unicode'],\n",
       "                                        'tfidf__sublinear_tf': [True]},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_svc = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"svc\", SVC(random_state=42))])\n",
    "\n",
    "params = {\n",
    "    'tfidf__sublinear_tf':[True], \n",
    "    'tfidf__strip_accents':['unicode'],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 2)], #(1,2)\n",
    "    'tfidf__min_df': [5], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'tfidf__analyzer':['word'],\n",
    "    'svc__C':[10**-2, 10**-1, 10**0, 10**1, 10**2],\n",
    "    'svc__kernel': ['linear', 'poly','rbf', 'sigmoid'],\n",
    "    'svc__gamma': [0.5,0.1,1]\n",
    "\n",
    "}\n",
    "   \n",
    "pipe_svc_clf = RandomizedSearchCV(pipe_svc, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", n_iter=10, verbose=1)\n",
    "pipe_svc_clf.fit(data_stemmed, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ca13187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8789690913235946\n",
      "best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 5, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word', 'svc__kernel': 'rbf', 'svc__gamma': 0.5, 'svc__C': 1}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_svc_clf.best_score_))\n",
    "print('best score {}'.format(pipe_svc_clf.best_params_))\n",
    "#best score 0.8789690913235946\n",
    "#{'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 5, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word', 'svc__kernel': 'rbf', 'svc__gamma': 0.5, 'svc__C': 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e35a6",
   "metadata": {},
   "source": [
    "i think svm would give better result than logestic but if i had enough resources and enough time its much easier to try tunning logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947c8013",
   "metadata": {},
   "source": [
    "#  lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f3125",
   "metadata": {},
   "source": [
    "## trial 3 Logistic regression with lemmatized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b8371fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('lg',\n",
       "                                              LogisticRegression(max_iter=10000,\n",
       "                                                                 n_jobs=-1,\n",
       "                                                                 random_state=42))]),\n",
       "                   n_iter=200, n_jobs=-1,\n",
       "                   param_distributions={'tfidf__analyzer': ['word', 'char',\n",
       "                                                            'char_wb'],\n",
       "                                        'tfidf__max_df': array([0.2]),\n",
       "                                        'tfidf__min_df': array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 23, 24, 25, 26, 27,...\n",
       "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
       "                                                               (1, 4), (1, 5)],\n",
       "                                        'tfidf__smooth_idf': [False, True],\n",
       "                                        'tfidf__strip_accents': [None, 'ascii',\n",
       "                                                                 'unicode'],\n",
       "                                        'tfidf__sublinear_tf': [True, False]},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_lg = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"lg\", LogisticRegression(max_iter=10000,random_state=42,n_jobs=-1))])\n",
    "\n",
    "\n",
    "# define parameter space to test\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
    "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    \"tfidf__analyzer\": ['word','char','char_wb'],\n",
    "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
    "    'tfidf__smooth_idf':[False,True],\n",
    "    \"tfidf__sublinear_tf\":[True,False]\n",
    "}\n",
    "\n",
    "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
    "\n",
    "pipe_lg_clf2 = RandomizedSearchCV(pipe_lg, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", n_iter=200, verbose=1)\n",
    "pipe_lg_clf2.fit(data_lemmatized, data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0fefd923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8796225479855799\n",
      "best params {'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 5, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_lg_clf2.best_score_))\n",
    "print('best params {}'.format(pipe_lg_clf2.best_params_))\n",
    "# best score 0.8796225479855799\n",
    "# best params {'tfidf__sublinear_tf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 4), 'tfidf__min_df': 5, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "499bafec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 192 candidates, totalling 960 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                       ('lg',\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           n_jobs=-1,\n",
       "                                                           random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'lg__C': [1.0, 0.1, 0.005, 1.5, 2.0, 3.5, 4, 5],\n",
       "                         'lg__class_weight': ['balanced', None],\n",
       "                         'lg__fit_intercept': [False, True],\n",
       "                         'lg__solver': ['sag', 'newton-cg', 'lbfgs',\n",
       "                                        'liblinear', 'saga', 'sag'],\n",
       "                         'tfidf__analyzer': ['word'], 'tfidf__max_df': [0.2],\n",
       "                         'tfidf__min_df': [5], 'tfidf__ngram_range': [(1, 4)],\n",
       "                         'tfidf__smooth_idf': [False],\n",
       "                         'tfidf__strip_accents': ['unicode'],\n",
       "                         'tfidf__sublinear_tf': [False]},\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_lg = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"lg\", LogisticRegression(max_iter=10000,random_state=42,n_jobs=-1))])\n",
    "\n",
    "params = {\n",
    "    'tfidf__sublinear_tf':[False],\n",
    "    'tfidf__strip_accents':[\"unicode\"],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 4)],\n",
    "    'tfidf__analyzer':['word'],\n",
    "    'tfidf__min_df': [5], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'lg__class_weight':['balanced',None],\n",
    "    \"lg__solver\" : ['sag','newton-cg', 'lbfgs','liblinear','saga', 'sag'],\n",
    "    'lg__C': [1.0,0.1,0.005,1.5,2.0,3.5,4,5],\n",
    "    'lg__fit_intercept':[False, True],\n",
    "\n",
    "}\n",
    "\n",
    "pipe_lg_clf2 = GridSearchCV(pipe_lg, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", verbose=1)\n",
    "pipe_lg_clf2.fit(data_lemmatized, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "eb50655f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8845699688823505\n",
      "best params {'lg__C': 2.0, 'lg__class_weight': 'balanced', 'lg__fit_intercept': False, 'lg__solver': 'newton-cg', 'tfidf__analyzer': 'word', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 4), 'tfidf__smooth_idf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__sublinear_tf': False}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_lg_clf2.best_score_))\n",
    "print('best params {}'.format(pipe_lg_clf2.best_params_))\n",
    "#best score 0.8845699688823505\n",
    "# best params {'lg__C': 2.0, 'lg__class_weight': 'balanced', 'lg__fit_intercept': False, 'lg__solver': 'newton-cg', 'tfidf__analyzer': 'word', 'tfidf__max_df': 0.2, 'tfidf__min_df': 5, 'tfidf__ngram_range': (1, 4), 'tfidf__smooth_idf': False, 'tfidf__strip_accents': 'unicode', 'tfidf__sublinear_tf': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "df1826e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59151\n"
     ]
    }
   ],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = test_lemmatized.index\n",
    "print(len(test['text']))\n",
    "submission['label'] = pipe_lg_clf2.predict_proba(test_lemmatized)[:,1]\n",
    "submission.to_csv('lg_lemmatized_finalGR_nodrpzero.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d184a640",
   "metadata": {},
   "source": [
    "## trial 4 SVM with lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a91d14",
   "metadata": {},
   "source": [
    "it didn't look promising and it will take very long time so i decided not to continue with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e759c4a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('svc', SVC(random_state=42))]),\n",
       "                   n_jobs=-1,\n",
       "                   param_distributions={'tfidf__analyzer': ['word', 'char',\n",
       "                                                            'char_wb'],\n",
       "                                        'tfidf__max_df': array([0.2]),\n",
       "                                        'tfidf__min_df': array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "       39, 40, 41, 42, 43, 44...48, 49, 50, 51, 52, 53, 54, 55,\n",
       "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
       "                                                               (1, 4), (1, 5)],\n",
       "                                        'tfidf__smooth_idf': [False, True],\n",
       "                                        'tfidf__strip_accents': [None, 'ascii',\n",
       "                                                                 'unicode'],\n",
       "                                        'tfidf__sublinear_tf': [True, False]},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_svc = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"svc\", SVC(random_state=42))])\n",
    "\n",
    "\n",
    "# define parameter space to test\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
    "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
    "    'tfidf__analyzer':['word','char','char_wb'],\n",
    "    'tfidf__smooth_idf':[False,True],\n",
    "    \"tfidf__sublinear_tf\":[True,False]\n",
    "}\n",
    "\n",
    "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
    "\n",
    "pipe_svc_clf2 = RandomizedSearchCV(pipe_svc, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", n_iter=10, verbose=1)\n",
    "pipe_svc_clf2.fit(data_lemmatized, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "3246aea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8794486916772799\n",
      "best score {'tfidf__sublinear_tf': False, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 11, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_svc_clf2.best_score_))\n",
    "print('best score {}'.format(pipe_svc_clf2.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8d8776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params = {\n",
    "#     'tfidf__sublinear_tf':[False], \n",
    "#     'tfidf__strip_accents':['unicode'],\n",
    "#     'tfidf__smooth_idf':[False],\n",
    "#     'tfidf__ngram_range': [(1, 5)], #(1,2)\n",
    "#     'tfidf__min_df': [6], \n",
    "#     'tfidf__max_df': [0.2],\n",
    "#     'tfidf__analyzer':['char'],\n",
    "#     'svc__C':[10**-2, 10**-1, 10**0, 10**1, 10**2],\n",
    "#     'svc__kernel': ['linear', 'poly','rbf', 'sigmoid'],\n",
    "#     'svc__gamma': [0.5,0.1,1]\n",
    "\n",
    "# }\n",
    "   \n",
    "# pipe_svc_clf2 = RandomizedSearchCV(pipe_svc, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", n_iter=20, verbose=1)\n",
    "# pipe_svc_clf2.fit(data_lemmatized, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4761ee85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('best score {}'.format(pipe_lg_clf2.best_score_))\n",
    "# print('best params {}'.format(pipe_lg_clf2.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c397b45",
   "metadata": {},
   "source": [
    "## trial 5 PassiveAggressiveClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3360a0",
   "metadata": {},
   "source": [
    "i tried to look for another classifier that can outpreform logistic regression but that was not the case :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b7138827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('pa',\n",
       "                                              PassiveAggressiveClassifier(random_state=42))]),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'tfidf__analyzer': ['word', 'char'],\n",
       "                                        'tfidf__max_df': array([0.2]),\n",
       "                                        'tfidf__min_df': array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36...\n",
       "       39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
       "                                                               (1, 4), (1, 5)],\n",
       "                                        'tfidf__smooth_idf': [False, True],\n",
       "                                        'tfidf__strip_accents': [None, 'ascii',\n",
       "                                                                 'unicode'],\n",
       "                                        'tfidf__sublinear_tf': [False, True]},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# feature creation and modelling in a single function\n",
    "pipe_pa = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"pa\", PassiveAggressiveClassifier(random_state=42))])\n",
    "\n",
    "\n",
    "# define parameter space to test\n",
    "params = {\n",
    "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5)],\n",
    "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    \"tfidf__analyzer\": ['word','char'],\n",
    "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
    "    'tfidf__smooth_idf':[False,True],\n",
    "    \"tfidf__sublinear_tf\":[False,True]\n",
    "}\n",
    "\n",
    "# here we still use data_lemmatized; but the random search model will use our predefined split internally to determine which sample belongs to the validation set\n",
    "\n",
    "pipe_pa_clf = RandomizedSearchCV(pipe_pa, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", n_iter=100, verbose=1)\n",
    "pipe_pa_clf.fit(data_stemmed, data['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1da3939c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.8486310776986358\n",
      "best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': None, 'tfidf__smooth_idf': True, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 15, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word'}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_pa_clf.best_score_))\n",
    "print('best score {}'.format(pipe_pa_clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7fcf0916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('pa',\n",
       "                                              PassiveAggressiveClassifier(random_state=42))]),\n",
       "                   n_iter=100, n_jobs=-1,\n",
       "                   param_distributions={'pa__C': [1.0, 0.1, 0.005, 1.5, 2.0,\n",
       "                                                  3.5, 4, 5, 10.0],\n",
       "                                        'pa__average': [False, True],\n",
       "                                        'pa__class_weight': ['balanced', None],\n",
       "                                        'pa__fit_intercept': [False, True],\n",
       "                                        'pa__loss': ['hinge', 'squared_hinge'],\n",
       "                                        'tfidf__analyzer': ['word'],\n",
       "                                        'tfidf__max_df': [0.2],\n",
       "                                        'tfidf__min_df': [14],\n",
       "                                        'tfidf__ngram_range': [(1, 2)],\n",
       "                                        'tfidf__smooth_idf': [False],\n",
       "                                        'tfidf__strip_accents': ['unicode'],\n",
       "                                        'tfidf__sublinear_tf': [True]},\n",
       "                   scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'tfidf__sublinear_tf':[True],\n",
    "    'tfidf__strip_accents':['unicode'],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 2)],\n",
    "    'tfidf__analyzer':['word'],\n",
    "    'tfidf__min_df': [14], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'pa__class_weight':['balanced',None],\n",
    "    \"pa__loss\" : ['hinge','squared_hinge'],\n",
    "    'pa__C': [1.0,0.1,0.005,1.5,2.0,3.5,4,5, 10.0],\n",
    "    'pa__fit_intercept':[False, True],\n",
    "    'pa__average':[False, True],\n",
    "}\n",
    "\n",
    "pipe_pa_clf = RandomizedSearchCV(pipe_pa, params, n_jobs=-1,cv=5, scoring=\"roc_auc\", n_iter=100, verbose=1)\n",
    "pipe_pa_clf.fit(data_stemmed, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bc627ffa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.875545069320939\n",
      "best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'unicode', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 2), 'tfidf__min_df': 14, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word', 'pa__loss': 'squared_hinge', 'pa__fit_intercept': True, 'pa__class_weight': 'balanced', 'pa__average': True, 'pa__C': 0.005}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_pa_clf.best_score_))\n",
    "print('best score {}'.format(pipe_pa_clf.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c7382a",
   "metadata": {},
   "source": [
    "## trial 6 Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28197f4",
   "metadata": {},
   "source": [
    "that was my kast option for machine learning classifiers i guess the best one for me was the logistic regression after all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e03e8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=5,\n",
       "                   estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                             ('clf', MultinomialNB())]),\n",
       "                   n_iter=200, n_jobs=-1,\n",
       "                   param_distributions={'clf__alpha': (0.1, 0.5, 1.0),\n",
       "                                        'tfidf__analyzer': ['word'],\n",
       "                                        'tfidf__max_df': array([0.2]),\n",
       "                                        'tfidf__min_df': array([ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21,\n",
       "       22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38,\n",
       "       3...44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55,\n",
       "       56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72,\n",
       "       73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89,\n",
       "       90, 91, 92, 93, 94, 95, 96, 97, 98, 99]),\n",
       "                                        'tfidf__ngram_range': [(1, 2), (1, 3),\n",
       "                                                               (1, 4), (1, 5),\n",
       "                                                               (2, 2)],\n",
       "                                        'tfidf__smooth_idf': [False, True],\n",
       "                                        'tfidf__strip_accents': [None, 'ascii',\n",
       "                                                                 'unicode'],\n",
       "                                        'tfidf__sublinear_tf': [False, True]},\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('clf', MultinomialNB())\n",
    "])\n",
    "\n",
    "# Define the parameters to be tuned\n",
    "parameters = {\n",
    "    \"tfidf__ngram_range\": [(1, 2), (1, 3), (1,4), (1,5),(2,2)],\n",
    "    \"tfidf__max_df\": np.arange(0.2, 1.0),\n",
    "    \"tfidf__min_df\": np.arange(5, 100),\n",
    "    \"tfidf__analyzer\": ['word'],\n",
    "    \"tfidf__strip_accents\":[None,'ascii','unicode'],\n",
    "    'tfidf__smooth_idf':[False,True],\n",
    "    \"tfidf__sublinear_tf\":[False,True],\n",
    "    \"clf__alpha\": (0.1, 0.5, 1.0)\n",
    "}\n",
    "grid_search = RandomizedSearchCV(pipeline, parameters, cv=5, n_jobs=-1, n_iter=200, verbose=1)\n",
    "grid_search.fit(data_lemmatized, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "14faa9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.7812333333333333\n",
      "best score {'tfidf__sublinear_tf': True, 'tfidf__strip_accents': 'ascii', 'tfidf__smooth_idf': False, 'tfidf__ngram_range': (1, 5), 'tfidf__min_df': 5, 'tfidf__max_df': 0.2, 'tfidf__analyzer': 'word', 'clf__alpha': 1.0}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(grid_search.best_score_))\n",
    "print('best score {}'.format(grid_search.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6766e1c",
   "metadata": {},
   "source": [
    "# trial 7 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e7870f",
   "metadata": {},
   "source": [
    "one last trial with no cross validation and using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a3bd63c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_2, X_val_2 ,Y_train_2, Y_val_2 = train_test_split(data_stemmed,data['label'],stratify=data['label'], random_state=42, test_size=0.25, shuffle=True)\n",
    "\n",
    "\n",
    "split_index_stemmed = [-1 if x in X_train_2.index else 0 for x in data_stemmed.index]\n",
    "ps = PredefinedSplit(split_index_stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a31379a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 1280 candidates, totalling 1280 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=PredefinedSplit(test_fold=array([-1, -1, ..., -1, -1])),\n",
       "             estimator=Pipeline(steps=[('tfidf', TfidfVectorizer()),\n",
       "                                       ('lg',\n",
       "                                        LogisticRegression(max_iter=10000,\n",
       "                                                           n_jobs=-1,\n",
       "                                                           random_state=42))]),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'lg__C': [1.0, 0.1, 0.001, 1.5, 2.0, 3.5, 4, 5],\n",
       "                         'lg__class_weight': ['balanced', None],\n",
       "                         'lg__fit_intercept': [False, True],\n",
       "                         'lg__solver': ['sag', 'newton-cg', 'lbfgs',\n",
       "                                        'liblinear', 'saga'],\n",
       "                         'tfidf__analyzer': ['char', 'word'],\n",
       "                         'tfidf__max_df': [0.2], 'tfidf__min_df': [11],\n",
       "                         'tfidf__ngram_range': [(1, 5), (1, 2)],\n",
       "                         'tfidf__smooth_idf': [False],\n",
       "                         'tfidf__strip_accents': [None],\n",
       "                         'tfidf__sublinear_tf': [True, False]},\n",
       "             scoring='roc_auc', verbose=1)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lg = Pipeline([(\"tfidf\", TfidfVectorizer()), (\"lg\", LogisticRegression(max_iter=10000,random_state=42,n_jobs=-1))])\n",
    "\n",
    "params = {\n",
    "    'tfidf__sublinear_tf':[True,False], \n",
    "    'tfidf__strip_accents':[None],\n",
    "    'tfidf__smooth_idf':[False],\n",
    "    'tfidf__ngram_range': [(1, 5),(1,2)], \n",
    "    'tfidf__analyzer':['char','word'],\n",
    "    'tfidf__min_df': [11], \n",
    "    'tfidf__max_df': [0.2],\n",
    "    'lg__class_weight':['balanced',None],\n",
    "    \"lg__solver\" : ['sag','newton-cg', 'lbfgs','liblinear','saga'],\n",
    "    'lg__C': [1.0,0.1,0.001,1.5,2.0,3.5,4,5],\n",
    "    'lg__fit_intercept':[False, True],\n",
    "\n",
    "}\n",
    "\n",
    "pipe_lg_clf = GridSearchCV(pipe_lg, params, n_jobs=-1,cv=ps, scoring=\"roc_auc\", verbose=1)\n",
    "pipe_lg_clf.fit(data_stemmed, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8ecafa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score 0.890334974227255\n",
      "best score {'lg__C': 3.5, 'lg__class_weight': 'balanced', 'lg__fit_intercept': True, 'lg__solver': 'sag', 'tfidf__analyzer': 'char', 'tfidf__max_df': 0.2, 'tfidf__min_df': 11, 'tfidf__ngram_range': (1, 5), 'tfidf__smooth_idf': False, 'tfidf__strip_accents': None, 'tfidf__sublinear_tf': True}\n"
     ]
    }
   ],
   "source": [
    "print('best score {}'.format(pipe_lg_clf.best_score_))\n",
    "print('best score {}'.format(pipe_lg_clf.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f9532b41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(analyzer='char', max_df=0.2, min_df=11,\n",
       "                                 ngram_range=(1, 5), smooth_idf=False,\n",
       "                                 sublinear_tf=True)),\n",
       "                ('lg',\n",
       "                 LogisticRegression(C=3.5, class_weight='balanced',\n",
       "                                    max_iter=10000, n_jobs=-1, random_state=42,\n",
       "                                    solver='sag'))])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_lg.set_params(**pipe_lg_clf.best_params_).fit(data_stemmed, data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c46c9fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59151\n"
     ]
    }
   ],
   "source": [
    "#create submission file\n",
    "submission = pd.DataFrame()\n",
    "\n",
    "submission['id'] = test_stemmed.index\n",
    "print(len(test['text']))\n",
    "submission['label'] = pipe_lg.predict_proba(test_stemmed)[:,1]\n",
    "submission.to_csv('bestLGfinal.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1820a87b",
   "metadata": {},
   "source": [
    "# Deep learning approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feb1595",
   "metadata": {},
   "source": [
    "##  Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "83826a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.optimizers import Nadam\n",
    "# Load the BERT model from TensorFlow Hub\n",
    "bert_model = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4')\n",
    "\n",
    "# Load the BERT preprocessing layer from TensorFlow Hub\n",
    "bert_preprocess = hub.KerasLayer('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_lemmatized,data[\"label\"],stratify=data[\"label\"],test_size = 0.05,random_state=1)\n",
    "# Define the input and output layers for the classifier\n",
    "text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text')\n",
    "encoder_inputs = bert_preprocess(text_input)\n",
    "encoder_outputs = bert_model(encoder_inputs)\n",
    "pooled_output = encoder_outputs['pooled_output']\n",
    "dropout = tf.keras.layers.Dropout(0.5)(pooled_output)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(dropout)\n",
    "\n",
    "# Create a Keras model for sentiment analysis\n",
    "model = tf.keras.Model(inputs=[text_input], outputs=[output])\n",
    "opt = Nadam(learning_rate=0.002)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, min_lr=0.001)\n",
    "# Compile the model with binary crossentropy loss and Adam optimizer\n",
    "model.compile(optimizer=opt,loss='binary_crossentropy',metrics=['AUC'])\n",
    "\n",
    "# # Assume data is a data frame with two columns: text and label\n",
    "# X_train = data['text']\n",
    "# y_train = data['label']\n",
    "early_stopping = EarlyStopping(monitor='val_auc', patience=3, verbose=1, mode='auto', restore_best_weights=True)\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train, validation_split=0.2, epochs=50, batch_size=32, callbacks=[early_stopping,reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "76101153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374/374 [==============================] - 72s 193ms/step - loss: 0.4114 - auc: 0.8981\n",
      "Test loss: 0.41137224435806274\n",
      "Test AUC: 0.8980545997619629\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_auc = model.evaluate(X_test, y_test)\n",
    "print('Test loss:', test_loss)\n",
    "print('Test AUC:', test_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0024a2de",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfe83fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      " 161/1495 [==>...........................] - ETA: 4:15 - loss: 0.8782 - auc: 0.6988"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Load the dataset\n",
    "df = pd.read_csv('xy_train.csv',index_col='id')\n",
    "df = df.drop(df[df.label == 2].index)\n",
    "# Tokenize the text and convert it to sequences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['text'])\n",
    "\n",
    "# Pad the sequences to a fixed length\n",
    "X = pad_sequences(sequences, maxlen=50)\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df['label'],stratify=df[\"label\"],test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the model with LSTM layers\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 128, input_length=50))\n",
    "model.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['AUC'])\n",
    "\n",
    "# Train the model with class weights\n",
    "\n",
    "# Train the model with class weights and early stopping\n",
    "history = model.fit(X_train, y_train, epochs=3, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predict the probabilities of the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "print(\"ROC AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e678b4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849/1849 [==============================] - 17s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "# Load the testing dataset\n",
    "test_df = pd.read_csv('x_test.csv')\n",
    "\n",
    "# Tokenize the text and convert it to sequences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(test_df['text'])\n",
    "sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "X_test = pad_sequences(sequences, maxlen=50)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Create a new pandas DataFrame with the predicted probabilities and the ID column\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['id'] = test_df['id']\n",
    "submission_df['label'] = y_pred\n",
    "\n",
    "# Save the submission DataFrame as a CSV file\n",
    "submission_df.to_csv(\"submissionfinaaaaaaaaal.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3f4a4",
   "metadata": {},
   "source": [
    "# lstm + cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "df94a69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "374/374 [==============================] - 68s 178ms/step - loss: 0.8560 - auc: 0.7874 - val_loss: 0.5218 - val_auc: 0.8749\n",
      "Epoch 2/50\n",
      "374/374 [==============================] - 62s 167ms/step - loss: 0.4873 - auc: 0.8946 - val_loss: 0.4948 - val_auc: 0.8867\n",
      "Epoch 3/50\n",
      "374/374 [==============================] - 63s 169ms/step - loss: 0.4429 - auc: 0.9199 - val_loss: 0.4876 - val_auc: 0.8896\n",
      "Epoch 4/50\n",
      "374/374 [==============================] - 65s 174ms/step - loss: 0.4141 - auc: 0.9343 - val_loss: 0.4811 - val_auc: 0.8912\n",
      "Epoch 5/50\n",
      "374/374 [==============================] - 66s 175ms/step - loss: 0.3926 - auc: 0.9439 - val_loss: 0.4863 - val_auc: 0.8887\n",
      "Epoch 6/50\n",
      "374/374 [==============================] - 62s 166ms/step - loss: 0.3741 - auc: 0.9518 - val_loss: 0.4907 - val_auc: 0.8913\n",
      "Epoch 7/50\n",
      "374/374 [==============================] - 62s 166ms/step - loss: 0.3595 - auc: 0.9575 - val_loss: 0.5047 - val_auc: 0.8894\n",
      "Epoch 8/50\n",
      "374/374 [==============================] - 63s 167ms/step - loss: 0.3453 - auc: 0.9629 - val_loss: 0.5251 - val_auc: 0.8880\n",
      "Epoch 9/50\n",
      "374/374 [==============================] - 63s 168ms/step - loss: 0.3341 - auc: 0.9669 - val_loss: 0.5235 - val_auc: 0.8875\n",
      "Epoch 10/50\n",
      "374/374 [==============================] - 63s 168ms/step - loss: 0.3218 - auc: 0.9709 - val_loss: 0.5238 - val_auc: 0.8861\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Dense, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the data\n",
    "train_df =  pd.read_csv('xy_train.csv',index_col='id')\n",
    "train_df = train_df.drop(train_df[train_df.label == 2].index)\n",
    "test_df = pd.read_csv('x_test.csv')\n",
    "# Convert the text column of the DataFrames to a list of strings\n",
    "train_texts = train_df['text'].to_numpy().tolist()\n",
    "test_texts = test_df['text'].to_numpy().tolist()\n",
    "\n",
    "# Convert the label column of the DataFrames to a NumPy array\n",
    "train_labels = train_df['label'].to_numpy()\n",
    "\n",
    "# Initialize a tokenizer with a vocabulary size of 10,000\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "\n",
    "# Fit the tokenizer on the training data\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Convert the texts to sequences of word indices\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Pad the sequences to a maximum length of 256\n",
    "max_length = 256\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(max_length,), dtype='int32')\n",
    "\n",
    "# Add an embedding layer to convert the input indices to dense vectors\n",
    "embedding_layer = Embedding(input_dim=10000, output_dim=128, input_length=max_length)(input_layer)\n",
    "\n",
    "# Add a LSTM layer with 64 units and L2 regularization\n",
    "lstm_layer = LSTM(units=64, kernel_regularizer=regularizers.l2(0.01))(embedding_layer)\n",
    "\n",
    "# Add a 1D convolutional layer with 128 filters and a kernel size of 3, and L2 regularization\n",
    "conv_layer = Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding_layer)\n",
    "\n",
    "# Add a global max pooling layer to extract the most important features\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "\n",
    "# Add a dropout layer to prevent overfitting\n",
    "dropout_layer = Dropout(rate=0.5)(lstm_layer)\n",
    "\n",
    "# Concatenate the outputs of the dropout and pooling layers\n",
    "concat_layer = concatenate([dropout_layer, pooling_layer])\n",
    "\n",
    "# Add a dense layer with L2 regularization for classification\n",
    "dense_layer = Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))(concat_layer)\n",
    "\n",
    "# Create the Keras model\n",
    "model = Model(inputs=input_layer, outputs=dense_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])\n",
    "\n",
    "# Train the model with class weights\n",
    "early_stopping = EarlyStopping(monitor='val_auc', patience=4, restore_best_weights=True)\n",
    "\n",
    "# Train the model with class weights and early stopping\n",
    "history = model.fit(x=train_data, y=train_labels, epochs=50, batch_size=128,validation_split=0.2, callbacks=[early_stopping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18ca0ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "374/374 [==============================] - 8s 20ms/step\n",
      "ROC AUC score on the validation set: 0.8912908127223991\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "val_preds = model.predict(train_data[int(len(train_data)*0.8):])[:, 0]\n",
    "val_labels = train_labels[int(len(train_labels)*0.8):]\n",
    "roc_auc = roc_auc_score(val_labels, val_preds)\n",
    "print('ROC AUC score on the validation set:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3770402c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849/1849 [==============================] - 39s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict the probabilities of the test set using the trained model\n",
    "y_pred = model.predict(test_data)[:, 0]\n",
    "\n",
    "# Create a new pandas DataFrame with the predicted probabilities and the ID column\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['id'] = test_df['id']\n",
    "submission_df['label'] = y_pred\n",
    "\n",
    "# Save the submission DataFrame as a CSV file\n",
    "submission_df.to_csv('submissionlstmcnn2fff.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3535f9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.548615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.267323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.389484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.719527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.374875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59146</th>\n",
       "      <td>59146</td>\n",
       "      <td>0.845922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59147</th>\n",
       "      <td>59147</td>\n",
       "      <td>0.658412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59148</th>\n",
       "      <td>59148</td>\n",
       "      <td>0.001467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59149</th>\n",
       "      <td>59149</td>\n",
       "      <td>0.299196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59150</th>\n",
       "      <td>59150</td>\n",
       "      <td>0.701185</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59151 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id     label\n",
       "0          0  0.548615\n",
       "1          1  0.267323\n",
       "2          2  0.389484\n",
       "3          3  0.719527\n",
       "4          4  0.374875\n",
       "...      ...       ...\n",
       "59146  59146  0.845922\n",
       "59147  59147  0.658412\n",
       "59148  59148  0.001467\n",
       "59149  59149  0.299196\n",
       "59150  59150  0.701185\n",
       "\n",
       "[59151 rows x 2 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c39cb157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1681/1681 [==============================] - 33s 19ms/step - loss: 0.6166 - auc: 0.8226 - val_loss: 0.4895 - val_auc: 0.8848 - lr: 0.0010\n",
      "Epoch 2/50\n",
      "1681/1681 [==============================] - 32s 19ms/step - loss: 0.4584 - auc: 0.8999 - val_loss: 0.5021 - val_auc: 0.8918 - lr: 0.0010\n",
      "Epoch 3/50\n",
      "1681/1681 [==============================] - 32s 19ms/step - loss: 0.4202 - auc: 0.9189 - val_loss: 0.4627 - val_auc: 0.8928 - lr: 0.0010\n",
      "Epoch 4/50\n",
      "1681/1681 [==============================] - 31s 19ms/step - loss: 0.3953 - auc: 0.9304 - val_loss: 0.4871 - val_auc: 0.8910 - lr: 0.0010\n",
      "Epoch 5/50\n",
      "1681/1681 [==============================] - 33s 19ms/step - loss: 0.3538 - auc: 0.9496 - val_loss: 0.4827 - val_auc: 0.8912 - lr: 2.0000e-04\n",
      "Epoch 6/50\n",
      "1681/1681 [==============================] - 32s 19ms/step - loss: 0.3473 - auc: 0.9526 - val_loss: 0.4839 - val_auc: 0.8918 - lr: 2.0000e-04\n",
      "Epoch 7/50\n",
      "1681/1681 [==============================] - 32s 19ms/step - loss: 0.3418 - auc: 0.9551 - val_loss: 0.4957 - val_auc: 0.8906 - lr: 2.0000e-04\n",
      "Epoch 8/50\n",
      "1681/1681 [==============================] - 32s 19ms/step - loss: 0.3319 - auc: 0.9592 - val_loss: 0.4996 - val_auc: 0.8904 - lr: 4.0000e-05\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Input, Embedding, LSTM, Conv1D, GlobalMaxPooling1D, Dense, concatenate, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the data\n",
    "train_df =  pd.read_csv('xy_train.csv',index_col='id')\n",
    "train_df = train_df.drop(train_df[train_df.label == 2].index)\n",
    "test_df = pd.read_csv('x_test.csv')\n",
    "# Convert the text column of the DataFrames to a list of strings\n",
    "train_texts = data_lemmatized\n",
    "test_texts = test_lemmatized\n",
    "\n",
    "# Convert the label column of the DataFrames to a NumPy array\n",
    "train_labels = train_df['label'].to_numpy()\n",
    "\n",
    "# Initialize a tokenizer with a vocabulary size of 10,000\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "\n",
    "# Fit the tokenizer on the training data\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "\n",
    "# Convert the texts to sequences of word indices\n",
    "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
    "test_sequences = tokenizer.texts_to_sequences(test_texts)\n",
    "\n",
    "# Pad the sequences to a maximum length of 256\n",
    "max_length = 256\n",
    "train_data = pad_sequences(train_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "test_data = pad_sequences(test_sequences, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Define the input layer\n",
    "input_layer = Input(shape=(max_length,), dtype='int32')\n",
    "\n",
    "# Add an embedding layer to convert the input indices to dense vectors\n",
    "embedding_layer = Embedding(input_dim=10000, output_dim=128, input_length=max_length)(input_layer)\n",
    "\n",
    "# Add a LSTM layer with 64 units and L2 regularization\n",
    "lstm_layer = LSTM(units=64, kernel_regularizer=regularizers.l2(0.01))(embedding_layer)\n",
    "\n",
    "# Add a 1D convolutional layer with 128 filters and a kernel size of 3, and L2 regularization\n",
    "conv_layer = Conv1D(filters=128, kernel_size=3, activation='relu', kernel_regularizer=regularizers.l2(0.01))(embedding_layer)\n",
    "\n",
    "# Add a global max pooling layer to extract the most important features\n",
    "pooling_layer = GlobalMaxPooling1D()(conv_layer)\n",
    "\n",
    "# Add a dropout layer to prevent overfitting\n",
    "dropout_layer = Dropout(rate=0.5)(lstm_layer)\n",
    "\n",
    "# Concatenate the outputs of the dropout and pooling layers\n",
    "concat_layer = concatenate([dropout_layer, pooling_layer])\n",
    "\n",
    "# Add a dense layer with L2 regularization for classification\n",
    "dense_layer = Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.01))(concat_layer)\n",
    "\n",
    "# Create the Keras model\n",
    "model = Model(inputs=input_layer, outputs=dense_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['AUC'])\n",
    "\n",
    "# Train the model with class weights\n",
    "early_stopping = EarlyStopping(monitor='val_auc', patience=5)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_auc', factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Train the model with class weights and early stopping\n",
    "history = model.fit(x=train_data, y=train_labels, epochs=50, batch_size=32,validation_split=0.1, callbacks=[early_stopping, reduce_lr])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e66286cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1849/1849 [==============================] - 19s 10ms/step\n"
     ]
    }
   ],
   "source": [
    "# Predict the probabilities of the test set using the trained model\n",
    "y_pred = model.predict(test_data)[:, 0]\n",
    "\n",
    "# Create a new pandas DataFrame with the predicted probabilities and the ID column\n",
    "submission_df = pd.DataFrame()\n",
    "submission_df['id'] = test_df['id']\n",
    "submission_df['label'] = y_pred\n",
    "\n",
    "# Save the submission DataFrame as a CSV file\n",
    "submission_df.to_csv('submissionlstmcnn2finallast.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b522db9",
   "metadata": {},
   "source": [
    "at the end it only got 0.83 on kaggle while svm got me 0.85 and logistic regression 0.84 that was maximum i could get from every approach :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad0b67b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
